{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 5: Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last worksheet, we finally finished implementing our neural network. It worked, but the results could have been better. However, in order to get better results, we probably would have needed to test out different network architectures or implement several advanced techniqes. That would take a lot of work -- implementing just our basic network was difficult enough. Thankfully though, there is a way to implement neural networks quickly and easily: *Machine Learning frameworks*.\n",
    "\n",
    "Using an ML framework simplifies the implementation of a neural network greatly. Frameworks will automatically derive all of the backpropagation calculations under without any help from the user. They usually also have pre-made implementations for different techniques, such as using special loss functions, different activation functions, different types of layers -- the list goes on. Oftentimes, the only thing a user has to do is specify the architecture of the model they want, set up the data pipeline, and train the network. No math necessary. Because of this, frameworks are super powerful; they are ubiquitous in academia and industry.\n",
    "\n",
    "There are many great ML frameworks, but the one we will learn about here is PyTorch. It is popular and easy to use, while still being flexible enough to give the user as much control as they want. Today, we will use PyTorch to create a powerful network that will completely outclass the one we implemented from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Tensors\n",
    "\n",
    "One of the most critical components of PyTorch is the *Torch Tensor*. For simplicity, you can think of the Torch Tensor as a fancy numpy array. The tensor is comprised of a multidimensional array, but it also stores important properties, such the previous mathematical operation applied to it (this is used for automatic gradient descent). Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instantiate torch tensors by passing in a Numpy Array.\n",
    "array1 = np.full((3, 4), 2)\n",
    "A = torch.Tensor(array1)\n",
    "print(A)\n",
    "print(A.shape)\n",
    "print(A[1, 2]) # we can index into torch tensors just like numpy arrays\n",
    "print()\n",
    "\n",
    "array2 = np.random.rand(3, 4)\n",
    "B = torch.Tensor(array2)\n",
    "B.requires_grad = True # we do this if we intend to calculate the gradients of this matrix\n",
    "print(B)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something interesting happens when we add these two tensors together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = A + B\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, torch saves the operation used to create C as \"grad_fn\". Now, lets try something new. Suppose we want  to minimize the first entry in C. It's sort of a stupid objective, but I want to illustrate a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = C[0, 0]\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the .backward() on a scalar tensor \"loss\" in order to calculate the gradients of \"B\" that could minimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(B.grad)\n",
    "\n",
    "loss.backward()\n",
    "print(B.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the .backward() method updated B.grad. We can then use this newly calculated gradient as we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = B - 0.1 * B.grad\n",
    "C = A + B\n",
    "print(C[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry of C got smaller! That was sort of a toy example, but it shows how the autograd feature of PyTorch works. Let's tackle the MNIST problem now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "In order to train a network in PyTorch, we will need to pass in our image and label data as tensors. Furthermore, we also have the option of \"mini-batching\" our data. For example, training with a mini-batch of size 20 means that we calculate the gradients for 20 image samples at a time, and then stepping in the average direction. This is different than how we trained originally, which was just stepping after calculating the gradient of one sample.\n",
    "\n",
    "This time, we will use mini-batches of size 20. That means we must feed our data in as a 3 dimensional tensor. The dimensions of our training images should be (number of minibatches, mini-batch size, num_features) = (N, 20, 784). The dimensions of our labels should be (N, 20, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data():\n",
    "    f = open('mnist_train.csv', 'r')\n",
    "    \n",
    "    lines = f.readlines()\n",
    "    \n",
    "    training_images = np.zeros((len(lines), 784))\n",
    "    training_labels = np.zeros((len(lines), 10))\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        label = int(line[0])\n",
    "        training_images[index, :] = np.fromstring(line[2:], dtype=int, sep=',')\n",
    "        training_labels[index, label - 1] = 1.0\n",
    "        index += 1\n",
    "        \n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    # now, instantiate torch tensors\n",
    "    training_images = torch.tensor(training_images, dtype=torch.float)\n",
    "    training_labels = torch.tensor(training_labels, dtype=torch.float)\n",
    "    \n",
    "    # reshape for minibatch size 20\n",
    "    # note that if num of total samples is not divisible by minibatch size, we may have to throw out some samples\n",
    "    training_images = training_images.view(-1, 20, 784)\n",
    "    training_labels = training_labels.view(-1, 20, 10)\n",
    "    \n",
    "    return training_images / 255, training_labels\n",
    "\n",
    "def get_validation_data():\n",
    "    f = open('mnist_test.csv', 'r')\n",
    "    \n",
    "    lines = f.readlines()\n",
    "    \n",
    "    val_images = np.zeros((len(lines), 784))\n",
    "    val_labels = np.zeros((len(lines), 10))\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        label = int(line[0])\n",
    "        val_images[index, :] = np.fromstring(line[2:], dtype=int, sep=',')\n",
    "        val_labels[index, label - 1] = 1.0\n",
    "        index += 1\n",
    "        \n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    val_images = torch.tensor(val_images, dtype=torch.float)\n",
    "    val_labels = torch.tensor(val_labels, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    val_images = val_images.view(-1, 20, 784)\n",
    "    val_labels = val_labels.view(-1, 20, 10)\n",
    "    \n",
    "    return val_images / 255, val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model\n",
    "So how do we create a neural network in pytorch? There are several options. We could use a very barebones approach and just instantiate our own tensors then run the autograd operation, sort of like we did above. However, a convenient and widely used approach would be to use torch.nn.Module. torch.nn.Module is an abstract class that we can inherit from to implement a neural network. All we have to do to use it is write the .\\__ init__() method and override the .forward() method. Once we do that, we have a new network Class -- we will be able to instantiate and train our own networks.\n",
    "\n",
    "Here is a basic network class as an example. Actually, this is a PyTorch implementation of the same network we made on our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        attributes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(784, 12) # these are \"linear layers\", just like the ones we \n",
    "                                                # implemented from scratch.\n",
    "        self.linear2 = torch.nn.Linear(12, 10)  # these layers each store a weight (and bias) tensor that \n",
    "                                                # can be updated (requires_grad=True)\n",
    "\n",
    "        self.tanh = torch.nn.Tanh() # the tanh activation function\n",
    "        self.sigmoid = torch.nn.Sigmoid() # the sigmoid activation\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function, we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a training loop to train our network. We have the option of using different loss functions and different optimizers. The optimizer provides a scheme with which to update the weights of the network. In the past, we used standard updating rule (step weights in the negative direction of the gradient). However, more sophisticated methods, such as involving the calculation of several moments of the gradient, have been shown to be more effective.\n",
    "\n",
    "However, we will simply stick to the most basic optimizer today (stochastic gradient descent).\n",
    "Observe how the training loop works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_images, training_labels, val_images, val_labels, epoch=5, lr=0.01):\n",
    "    \"\"\"\n",
    "    Trains a pytorch model using the given data.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr) # stochastic gradient descent. We must pass in the model's \n",
    "                                                  # updatable parameters into our optimizer.\n",
    "    loss_function = nn.MSELoss() # we use mean squared loss, same as before\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        model.train() # sets the model to training mode\n",
    "        for n in range(training_images.shape[0]):\n",
    "            x = training_images[n] # select a batch of images\n",
    "            y = training_labels[n] # select the corresponding batch of labels\n",
    "            \n",
    "            out = model(x) # this implicitly calls the .forward() method of model and returns the output\n",
    "            \n",
    "            loss = loss_function(out, y) # calculates loss given the output and label\n",
    "            \n",
    "            loss.backward() # fills gradient tensors for all tensors in each of the model's layers\n",
    "            \n",
    "            optimizer.step() # updates all parameters of the model\n",
    "            \n",
    "            optimizer.zero_grad() # Clears gradient tensors. This must be done after every update\n",
    "        \n",
    "        print(\"Epoch {} complete\".format(e + 1))\n",
    "        print(\"Evaluating the model...\")\n",
    "        evaluate(model, val_images, val_labels)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "def evaluate(model, val_images, val_labels):\n",
    "    model.eval() # sets the model to evaluation mode\n",
    "    num_correct = 0\n",
    "    total_num = val_labels.shape[0] * val_labels.shape[1]\n",
    "    with torch.no_grad():\n",
    "        for n in range(val_images.shape[0]):\n",
    "            x = training_images[n] \n",
    "            y = training_labels[n]\n",
    "            y = y.numpy()\n",
    "            truth = np.argmax(y, axis=1)\n",
    "            \n",
    "\n",
    "            out = model(x)\n",
    "            out = out.numpy()\n",
    "            out = np.argmax(out, axis=1)\n",
    "            results = np.equal(out, truth)\n",
    "            num_correct += np.sum(results)\n",
    "    print(\"    Percent correct: {}\".format(num_correct * 100 / total_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images, training_labels = get_training_data()\n",
    "val_images, val_labels = get_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicNet()\n",
    "train(model, training_images, training_labels, val_images, val_labels, epoch=10, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so these results are about the same as the network we coded on our own, as we should expect. Now, try to make a new neural network class. This time, it should have an extra hidden layer. The first hidden layer should have 400 nodes, and the second hidden layer should have 100 nodes. Use the nn.Module abstract class. Reference the [torch.nn documentation](https://pytorch.org/docs/stable/nn.html) for the layer and activation modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ### Add your code here\n",
    "        \n",
    "        ###\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ### Add your code here\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed how easy that was. Now, try using the Adam optimizer (reference the [optim documentation](https://pytorch.org/docs/stable/optim.html)) -- it is a more sophisticated optimizer that calculates several moments of the gradient. You can edit the existing train function or write another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write another neural network, this time with 3 hidden layers. Use whatever activation functions you want (look in the torch documentation). You can decide how many nodes in each layer it should have. See how high you can make the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
