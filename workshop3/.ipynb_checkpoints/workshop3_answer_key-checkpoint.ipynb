{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 3 Answer Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_data(path):\n",
    "    f = open(path, 'r')\n",
    "    \n",
    "    lines = f.readlines()\n",
    "    \n",
    "    training_images = np.zeros((len(lines), 784))\n",
    "    training_labels = np.zeros((len(lines), 10))\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        label = int(line[0])\n",
    "        training_images[index, :] = np.fromstring(line[2:], dtype=int, sep=',')\n",
    "        training_labels[index, label - 1] = 1.0\n",
    "        index += 1\n",
    "        \n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return training_images / 255, training_labels\n",
    "\n",
    "training_images, training_labels = get_data(\"../data/mnist_test.csv\")\n",
    "sample_image, sample_label = training_images[0], training_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method for sigmoid function\n",
    "def sigmoid(x):\n",
    "    return np.where(x >= 0, \n",
    "                    1 / (1 + np.exp(-x)), \n",
    "                    np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    A Fully Connected Neural Network. There are 784 input layer nodes, 12 hidden layer nodes, and 10 output layer\n",
    "    nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Arrays to hold node values\n",
    "        self.N = np.zeros((784, ))\n",
    "        self.H = np.zeros((12, ))\n",
    "        self.Z = np.zeros((10, ))\n",
    "        \n",
    "        # Arrays to hold weight values (randomly initialized between -1 and 1)\n",
    "        self.W = 2 * np.random.rand(784, 12) - 1\n",
    "        self.V = 2 * np.random.rand(12, 10) - 1\n",
    "        \n",
    "        # Arrays to hold biases for hidden and output nodes\n",
    "        self.B = 2 * np.random.rand(12) - 1\n",
    "        self.C = 2 * np.random.rand(10) - 1\n",
    "        \n",
    "        # Arrays to hold the gradients\n",
    "        self.W_grad = np.zeros((784, 12))\n",
    "        self.V_grad = np.zeros((12, 10))\n",
    "        self.B_grad = np.zeros((12, ))\n",
    "        self.C_grad = np.zeros((10, ))\n",
    "        \n",
    "\n",
    "    def fill_input_nodes(self, x):\n",
    "        \"\"\"\n",
    "        Fills input layer with image x.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.N = x\n",
    "        \n",
    "        \n",
    "    def fill_hidden_nodes(self):\n",
    "        \"\"\"\n",
    "        Fills the hidden layer nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        H = self.N @ self.W + self.B\n",
    "        self.H = np.tanh(H)\n",
    "        \n",
    "    def fill_output_nodes(self):\n",
    "        \"\"\"\n",
    "        Fills the output layer nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        Z = self.H @ self.V + self.C\n",
    "        self.Z = sigmoid(Z)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Given an image vector x, fills every node in the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.fill_input_nodes(x)\n",
    "        self.fill_hidden_nodes()\n",
    "        self.fill_output_nodes()\n",
    "\n",
    "    def calculate_loss(self, x, label):\n",
    "        \"\"\"\n",
    "        Given an image vector and its corresponding label vector, calculate the loss.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        label: input vector representing the label, a one-dimensional vector. Has a 1 in the position \n",
    "               corresponding to the correct answer, and 0s everywhere else.\n",
    "       \n",
    "        Returns:\n",
    "        loss: loss of the network given an image, label pair\n",
    "        \"\"\"\n",
    "        \n",
    "        out = self.forward(x)\n",
    "        loss = np.sum((self.Z - label) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    #### START WORKING ####\n",
    "    \n",
    "    \n",
    "    def calculate_dL_dVij(self, i, j, label):\n",
    "        \"\"\"\n",
    "        Use the formula you derived to calculate dL/dVi,j.\n",
    "        Place the result in the appropriate spot in self.V_grad.\n",
    "        \n",
    "        Parameters:\n",
    "        i, j: the indices telling which weight to calculate the partial derivative with respect to\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        dz = 2 * (self.Z[j] - label[j])\n",
    "        val = self.H @ self.V[:, j] + self.C[j]  # Summation value\n",
    "        dv = self.H[i] * sigmoid(val) * (1 - sigmoid(val))\n",
    "        self.V_grad[i, j] = dz * dv\n",
    "        \n",
    "    def calculate_V_grad(self, label):\n",
    "        \"\"\"\n",
    "        Use the function you just wrote to fill the entire self.V_grad array.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(12):\n",
    "            for j in range(10):\n",
    "                self.calculate_dL_dVij(i, j, label)\n",
    "        \n",
    "    def calculate_dL_dCj(self, j, label):\n",
    "        \"\"\"\n",
    "        Use the formula you derived to calculate dL/dCj.\n",
    "        Place the result in the appropriate spot in self.C_grad.\n",
    "        \n",
    "        Parameters:\n",
    "        j: the index telling which weight to calculate the partial derivative with respect to\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        dz = 2 * (self.Z[j] - label[j])\n",
    "        val = self.H @ self.V[:, j] + self.C[j]\n",
    "        dc = sigmoid(val) * (1 - sigmoid(val))\n",
    "        self.C_grad[j] = dz * dc\n",
    "        \n",
    "    def calculate_C_grad(self, label):\n",
    "        \"\"\"\n",
    "        Use the function you just wrote to fill the entire self.C_grad array.\n",
    "        \"\"\"\n",
    "        \n",
    "        for j in range(10):\n",
    "            self.calculate_dL_dCj(j, label)\n",
    "        \n",
    "    def calculate_dL_dWij(self, i, j, label):\n",
    "        \"\"\"\n",
    "        Use the formula you derived to calculate dL/dWi,j.\n",
    "        Place the result in the appropriate spot in self.W_grad.\n",
    "        \n",
    "        At this point, it may be helpful to go back into the .calculate_dL_dVij() method and save some values.\n",
    "        We will need to reuse some values, so it may not make sense to calculate it all over again.\n",
    "        \n",
    "        \n",
    "        Parameters:\n",
    "        i, j: the indices telling which weight to calculate the partial derivative with respect to\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        dz = 2 * (self.Z - label)\n",
    "        val_1 = self.H @ self.V + self.C\n",
    "        dh = self.V[j, :] * sigmoid(val_1) * (1 - sigmoid(val_1))\n",
    "        dh = dz @ dh\n",
    "        \n",
    "        val_2 = self.N @ self.W[:, j] + self.B[j]\n",
    "        dw = self.N[i] * (1 / (np.cosh(val_2) ** 2))\n",
    "        \n",
    "        self.W_grad[i, j] = dh * dw\n",
    "        \n",
    "    def calculate_W_grad(self, label):\n",
    "        \"\"\"\n",
    "        Use the function you just wrote to fill the entire self.W_grad array.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(784):\n",
    "            for j in range(12):\n",
    "                self.calculate_dL_dWij(i, j, label)\n",
    "        \n",
    "    def calculate_dL_dBj(self, j, label):\n",
    "        \"\"\"\n",
    "        Use the formula you derived to calculate dL/dBj.\n",
    "        Place the result in the appropriate spot in self.B_grad.\n",
    "        \n",
    "        Parameters:\n",
    "        j: the index telling which weight to calculate the partial derivative with respect to\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        dz = 2 * (self.Z - label)\n",
    "        val_1 = self.H @ self.V + self.C\n",
    "        dh = self.V[j, :] * sigmoid(val_1) * (1 - sigmoid(val_1))\n",
    "        dh = dz @ dh\n",
    "        \n",
    "        val_2 = self.N @ self.W[:, j] + self.B[j]\n",
    "        db = (1 / (np.cosh(val_2) ** 2))\n",
    "        \n",
    "        self.B_grad[j] = dh * db\n",
    "        \n",
    "    def calculate_B_grad(self, label):\n",
    "        \"\"\"\n",
    "        Use the function you just wrote to fill the entire self.B_grad array.\n",
    "        \"\"\"\n",
    "        \n",
    "        for j in range(12):\n",
    "            self.calculate_dL_dBj(j, label)\n",
    "        \n",
    "    def backpropagate(self, label):\n",
    "        \"\"\"\n",
    "        Perform backprop and fill gradient arrays.\n",
    "        You should not call this method before calling self.forward()\n",
    "        \n",
    "        Parameters:\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.calculate_V_grad(label)\n",
    "        self.calculate_C_grad(label)\n",
    "        self.calculate_W_grad(label)\n",
    "        self.calculate_B_grad(label)\n",
    "    \n",
    "    def update(self, lr=0.01):\n",
    "        \"\"\"\n",
    "        Use the full gradient arrays to update the weights and biases.\n",
    "        \n",
    "        Parameters:\n",
    "        lr: the learning rate. I set it to 0.01 by default, but we may later find other values to be better.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.V -= lr * self.V_grad\n",
    "        self.C -= lr * self.C_grad\n",
    "        self.W -= lr * self.W_grad\n",
    "        self.B -= lr * self.B_grad\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #### STOP WORKING ####\n",
    "    \n",
    "    \n",
    "    ### NOTE\n",
    "    # Once we calculate the weight and bias gradients, how can we check that they are correct?\n",
    "    # Thankfully there is a way to be sure if our code is correct. We take inspiration from the formal \n",
    "    # definition of a partial derivative -- the rate of change over an infinitesimally small interval.\n",
    "    # Thus, we can come up with an estimate of our partial by taking the rate of change over a small interval.\n",
    "    # Our calculated partial should match.\n",
    "    \n",
    "    # Each of these methods will select a random partial from its respective gradient array and compare it\n",
    "    # with the estimate. It will print both the calculated value and the estimated value.\n",
    "    # If the two numbers printed are close to the same, then your gradient calculations are correct.\n",
    "    # If they are very different, your calculations are wrong.\n",
    "    # Feel free to use these to test your implementations.\n",
    "    \n",
    "    \n",
    "    # Parameters:\n",
    "    # x: sample image\n",
    "    # y: sample label\n",
    "\n",
    "    def check_V_grad(self, x, y, perturb=0.00001):\n",
    "        randi = (random.randrange(12), random.randrange(10))\n",
    "        self.forward(x)\n",
    "        self.calculate_dL_dVij(randi[0], randi[1], y)\n",
    "        test1 = self.V_grad[randi[0], randi[1]]\n",
    "        self.clear()\n",
    "        loss1 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.V[randi[0], randi[1]] += perturb\n",
    "        loss2 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.V[randi[0], randi[1]] -= perturb\n",
    "        \n",
    "        test2 = (loss2 - loss1) / perturb\n",
    "        \n",
    "        print(test1)\n",
    "        print(test2)\n",
    "        \n",
    "    def check_W_grad(self, x, y, perturb=0.00001):\n",
    "        randi = (random.randrange(784), random.randrange(12))\n",
    "        self.forward(x)\n",
    "        self.calculate_dL_dWij(randi[0], randi[1], y)\n",
    "        test1 = self.W_grad[randi[0], randi[1]]\n",
    "        self.clear()\n",
    "        loss1 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.W[randi[0], randi[1]] += perturb\n",
    "        loss2 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.W[randi[0], randi[1]] -= perturb\n",
    "        \n",
    "        test2 = (loss2 - loss1) / perturb\n",
    "        \n",
    "        print(test1)\n",
    "        print(test2)\n",
    "        \n",
    "    \n",
    "    def check_C_grad(self, x, y, perturb=0.00001):\n",
    "        randi = random.randrange(10)\n",
    "        self.forward(x)\n",
    "        self.calculate_dL_dCj(randi, y)\n",
    "        test1 = self.C_grad[randi]\n",
    "        self.clear()\n",
    "        loss1 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.C[randi] += perturb\n",
    "        loss2 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.C[randi] -= perturb\n",
    "        \n",
    "        test2 = (loss2 - loss1) / perturb\n",
    "        \n",
    "        print(test1)\n",
    "        print(test2)\n",
    "        \n",
    "    def check_B_grad(self, x, y, perturb=0.00001):\n",
    "        randi = random.randrange(12)\n",
    "        self.forward(x)\n",
    "        self.calculate_dL_dBj(randi, y)\n",
    "        test1 = self.B_grad[randi]\n",
    "        self.clear()\n",
    "        loss1 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.B[randi] += perturb\n",
    "        loss2 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.B[randi] -= perturb\n",
    "        \n",
    "        test2 = (loss2 - loss1) / perturb\n",
    "        \n",
    "        print(test1)\n",
    "        print(test2)\n",
    "        \n",
    "        \n",
    "    def clear(self):\n",
    "        # Arrays to hold node values\n",
    "        self.N = np.zeros((784, ))\n",
    "        self.H = np.zeros((12, ))\n",
    "        self.Z = np.zeros((10, ))\n",
    "        \n",
    "        self.W_grad = np.zeros((784, 12))\n",
    "        self.V_grad = np.zeros((12, 10))\n",
    "        self.B_grad = np.zeros((12, ))\n",
    "        self.C_grad = np.zeros((10, ))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put each of the calls for checking different grads in seperate cells. This makes it easy to rerun different grad checks individually. You will want to run each gradient check multiple times to be more confident that all of the partials in the array are calculated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test gradient implementation for V:\n",
      "0.17262204434988426\n",
      "0.17262281195584703\n"
     ]
    }
   ],
   "source": [
    "print(\"Test gradient implementation for V:\")\n",
    "net.check_V_grad(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test gradient implementation for C:\n",
      "0.036758668528834026\n",
      "0.036758955168636476\n"
     ]
    }
   ],
   "source": [
    "print(\"Test gradient implementation for C:\")\n",
    "net.check_C_grad(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test gradient implementation for W:\n",
      "-0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Test gradient implementation for W:\")\n",
    "net.check_W_grad(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test gradient implementation for B:\n",
      "8.510464325136293e-05\n",
      "8.510379068127348e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Test gradient implementation for B:\")\n",
    "net.check_B_grad(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample loss before backpropagating: 2.8319902704951\n",
      "The sample loss after backpropagating and updating: 2.3153772747376458\n"
     ]
    }
   ],
   "source": [
    "net.forward(sample_image)\n",
    "loss1 = np.sum((net.Z - sample_label) ** 2)\n",
    "print(\"The sample loss before backpropagating: {}\".format(loss1))\n",
    "\n",
    "net.backpropagate(sample_label)\n",
    "net.update(lr=0.1)\n",
    "net.forward(sample_image)\n",
    "loss2 = np.sum((net.Z - sample_label) ** 2)\n",
    "print(\"The sample loss after backpropagating and updating: {}\".format(loss2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Note:\n",
    "\n",
    "If you were to try to train your network using the code we just wrote, it would be very slow. This is because the implementation we wrote included many loops and wasted time calculating values that we could have saved during the forward pass of the network. In order to make our code more efficient, we would have to save some more values during forward pass to use for the backprop calculations -- particularly the node values before applying activation function. We would also need to *vectorize* our code -- this involves using matrix operations to encapsulate all of our code, rather than using loops, similar to how we did with the forward pass.\n",
    "\n",
    "If you feel up to the task, you can read about [vectorization of gradient calculation](https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf) and give it a shot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
