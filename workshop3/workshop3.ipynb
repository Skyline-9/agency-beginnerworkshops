{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 3:  Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_data(path):\n",
    "    f = open(path, 'r')\n",
    "    \n",
    "    lines = f.readlines()\n",
    "    \n",
    "    training_images = np.zeros((len(lines), 784))\n",
    "    training_labels = np.zeros((len(lines), 10))\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        label = int(line[0])\n",
    "        training_images[index, :] = np.fromstring(line[2:], dtype=int, sep=',')\n",
    "        training_labels[index, label - 1] = 1.0\n",
    "        index += 1\n",
    "        \n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return training_images / 255, training_labels\n",
    "\n",
    "training_images, training_labels = get_data(\"mnist_test.csv\")\n",
    "sample_image, sample_label = training_images[0], training_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Neural Network\n",
    "Last week, we implemented the forward pass of our neural network. As a refresher, here is a diagram of our network:\n",
    "<img src=\"pics/FCN_2.png\" width=\"600\">\n",
    "For learning backpropagation, it is helpful to be familiar with the letters and notation we used last time to represent different parts of the network. Take some time to examine the diagram and recollect if you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "You probably noticed the box with an L inside it. That figure represents the loss, or error, of our network after forward propagation. The loss compares the network's output to the label that we wanted to see, and gives a measurement of how \"wrong\" the network was. For our network, we will simply take loss to be the distance between the output vector (Z) and the label for our given sample. The formula is below:\n",
    "<img src=\"pics/Loss.png\" width=\"600\">\n",
    "Our goal is to tweak the network's weights until this loss is minimized. If we can accomplish this, then our network will have effectively learned how to read hand-written digits. Think about it: suppose we show the network 80 percent of the total MNIST images, and we succesfully minimize the loss for these images. This means that for these samples, the network's output vector is very close to the given label vector. Then, when we show the network new images from the 20 percent that the network hasn't seen, maybe we can be confident that our network's outputs are very close to the actual labels. Thus, we will be able to predict the label for that new sample without being given the label -- just pick the answer closest to the generated output vector.\n",
    "\n",
    "So how do we minimize the loss? As it turns out, this can be solved with a useful calculus concept -- gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "We will now start getting into the math of backpropagation. In order to proceed, you will need to understand the multivariable calculus concepts of partial derivatives and gradients.\n",
    "\n",
    "To start, lets consider the weight matrix V. Suppose we forward pass an image into our network and then generate some loss. We want to tweak the weights in V in just the right way such that the loss becomes smaller. How can we use multivariable calculus to help us?\n",
    "\n",
    "If we consider the loss as a multivariable function of V, then we could calculate the gradient of the loss with respect to V.\n",
    "<img src=\"pics/Gradient.png\" width=\"600\">\n",
    "From calculus, we know that this gradient will point in the direction of greatest increase of the loss -- and the opposite of the gradient will point in the direction of greatest decrease. Then, we can update V as follows:\n",
    "<img src=\"pics/Update.png\" width=\"600\">\n",
    "This update rule basically shifts the weights of V by a small amount in the opposite direction of the gradient. The constant coefficient is the learning rate; it should be a small positive number. We step only a small amount because the gradient only tells us the direction of greatest increase -- not how far we should step. Thus, we step a small amount to be safe. This process is gradient descent.\n",
    "\n",
    "So how can we calculate the gradient of V? We will need to calculate the partial derivative of L with respect to every weight v[i, j]. But for now, let's start by trying to calculate the partial derivative with respect to weight v[1, 1]. Observe the diagram below:\n",
    "\n",
    "<img src=\"pics/Backprop_1.png\" width=\"600\">\n",
    "We see that the loss L is a function of z[1], z[2], ... , z[10]. However, weight v[1, 1] only influences z[1]. It does not influence z[2] through z[10]. Thus, we can use chain rule:\n",
    "<img src=\"pics/Chain_1.png\" width=\"600\">\n",
    "Then, we can find the partial derivative of L with respect to V[1, 1]:\n",
    "<img src=\"pics/Partial_1.png\" width=\"600\">\n",
    "So now that we have succesfully calculated the partial derivative with respect to V[1, 1], how do we compute the same for any V[i, j]? See if you can write down the formula on a sheet of paper.\n",
    "<img src=\"pics/Problem_1.png\" width=\"600\">\n",
    "Next, can you calculate the partial derivative with respect to any output bias C[j]? This should be an easier task.\n",
    "<img src=\"pics/Problem_2.png\" width=\"600\">\n",
    "Great! So we've calculated the necessary values to update the weight array V and bias array C. However, we still need to calculate the gradients for weight array W and bias array B. These calculations will be slightly different, because W and B further away from the end of the network.\n",
    "\n",
    "Let's try to calculate the partial derivative with respect to w[1, 1]. Observe the diagram below:\n",
    "<img src=\"pics/Backprop_2.png\" width=\"600\">\n",
    "Again, loss L is a function of z[1], z[2], ... , z[10]. But then, each z[i] is a function of h[1]. Finally, h[1] is a function of w[1, 1]. Knowing this, we can apply the chain rule as follows:\n",
    "<img src=\"pics/Chain_2.png\" width=\"600\">\n",
    "This may seem like a large computation, but if you recall, we have already calculated dL/dz[k] for all k = 1, ... , 10.\n",
    "Thus, we have the following:\n",
    "<img src=\"pics/Partial_2.png\" width=\"600\">\n",
    "Wow, that looks intimidating. Maybe copy this down but replace components of the equation by certain symbols, whatever makes it easier for you to digest.\n",
    "Now, calculate the partial derivative with respect to any weight w[i, j].\n",
    "<img src=\"pics/Problem_3.png\" width=\"600\">\n",
    "Calculate the partial derivative with respect to any bias b[i].\n",
    "<img src=\"pics/Problem_4.png\" width=\"600\">\n",
    "Now that we have the necessary formulas for gradient descent, we are ready to code. We will pick up where we left off last time. The forward pass methods are all complete, but you must now fill in the backpropagation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method for sigmoid function\n",
    "def sigmoid(x):\n",
    "    return np.where(x >= 0, \n",
    "                    1 / (1 + np.exp(-x)), \n",
    "                    np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    A Fully Connected Neural Network. There are 784 input layer nodes, 12 hidden layer nodes, and 10 output layer\n",
    "    nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Arrays to hold node values\n",
    "        self.N = np.zeros((784, ))\n",
    "        self.H = np.zeros((12, ))\n",
    "        self.Z = np.zeros((10, ))\n",
    "        \n",
    "        # Arrays to hold weight values (randomly initialized between -1 and 1)\n",
    "        self.W = 2 * np.random.rand(784, 12) - 1\n",
    "        self.V = 2 * np.random.rand(12, 10) - 1\n",
    "        \n",
    "        # Arrays to hold biases for hidden and output nodes\n",
    "        self.B = 2 * np.random.rand(12) - 1\n",
    "        self.C = 2 * np.random.rand(10) - 1\n",
    "        \n",
    "        # Arrays to hold the gradients\n",
    "        self.W_grad = np.zeros((784, 12))\n",
    "        self.V_grad = np.zeros((12, 10))\n",
    "        self.B_grad = np.zeros((12, ))\n",
    "        self.C_grad = np.zeros((10, ))\n",
    "        \n",
    "\n",
    "    def fill_input_nodes(self, x):\n",
    "        \"\"\"\n",
    "        Fills input layer with image x.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.N = x\n",
    "        \n",
    "        \n",
    "    def fill_hidden_nodes(self):\n",
    "        \"\"\"\n",
    "        Fills the hidden layer nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        H = np.dot(self.N, self.W) + self.B\n",
    "        self.H = np.tanh(H)\n",
    "        \n",
    "    def fill_output_nodes(self):\n",
    "        \"\"\"\n",
    "        Fills the output layer nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        Z = np.dot(self.H, self.V) + self.C\n",
    "        self.Z = sigmoid(Z)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Given an image vector x, fills every node in the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.fill_input_nodes(x)\n",
    "        self.fill_hidden_nodes()\n",
    "        self.fill_output_nodes()\n",
    "\n",
    "    def calculate_loss(self, x, label):\n",
    "        \"\"\"\n",
    "        Given an image vector and its corresponding label vector, calculate the loss.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        label: input vector representing the label, a one-dimensional vector. Has a 1 in the position \n",
    "               corresponding to the correct answer, and 0s everywhere else.\n",
    "       \n",
    "        Returns:\n",
    "        loss: loss of the network given an image, label pair\n",
    "        \"\"\"\n",
    "        \n",
    "        out = self.forward(x)\n",
    "        loss = np.sum((self.Z - label) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    #### START WORKING ####\n",
    "    \n",
    "    \n",
    "    def calculate_dL_dVij(self, i, j, label):\n",
    "        \"\"\"\n",
    "        Use the formula you derived to calculate dL/dVi,j.\n",
    "        Place the result in the appropriate spot in self.V_grad.\n",
    "        \n",
    "        Parameters:\n",
    "        i, j: the indices telling which weight to calculate the partial derivative with respect to\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "    def calculate_V_grad(self):\n",
    "        \"\"\"\n",
    "        Use the function you just wrote to fill the entire self.V_grad array.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "    def calculate_dL_dCj(self, j, label):\n",
    "        \"\"\"\n",
    "        Use the formula you derived to calculate dL/dCj.\n",
    "        Place the result in the appropriate spot in self.C_grad.\n",
    "        \n",
    "        Parameters:\n",
    "        j: the index telling which weight to calculate the partial derivative with respect to\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "    def calculate_C_grad(self):\n",
    "        \"\"\"\n",
    "        Use the function you just wrote to fill the entire self.C_grad array.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "    def calculate_dL_dWij(self, i, j, label):\n",
    "        \"\"\"\n",
    "        Use the formula you derived to calculate dL/dWi,j.\n",
    "        Place the result in the appropriate spot in self.W_grad.\n",
    "        \n",
    "        At this point, it may be helpful to go back into the .calculate_dL_dVij() method and save some values.\n",
    "        We will need to reuse some values, so it may not make sense to calculate it all over again.\n",
    "        \n",
    "        \n",
    "        Parameters:\n",
    "        i, j: the indices telling which weight to calculate the partial derivative with respect to\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "    def calculate_W_grad(self):\n",
    "        \"\"\"\n",
    "        Use the function you just wrote to fill the entire self.W_grad array.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "    def calculate_dL_dBj(self, j, label):\n",
    "        \"\"\"\n",
    "        Use the formula you derived to calculate dL/dBj.\n",
    "        Place the result in the appropriate spot in self.B_grad.\n",
    "        \n",
    "        Parameters:\n",
    "        j: the index telling which weight to calculate the partial derivative with respect to\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "    def calculate_B_grad(self):\n",
    "        \"\"\"\n",
    "        Use the function you just wrote to fill the entire self.B_grad array.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "    def backpropagate(self, label):\n",
    "        \"\"\"\n",
    "        Perform backprop and fill gradient arrays.\n",
    "        You should not call this method before calling self.forward()\n",
    "        \n",
    "        Parameters:\n",
    "        label: a label vector indicating the \"correct\" answer for the network\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "    \n",
    "    def update(self, lr=0.01):\n",
    "        \"\"\"\n",
    "        Use the full gradient arrays to update the weights and biases.\n",
    "        \n",
    "        Parameters:\n",
    "        lr: the learning rate. I set it to 0.01 by default, but we may later find other values to be better.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### TODO: Write the method\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #### STOP WORKING ####\n",
    "    \n",
    "    \n",
    "    ### NOTE\n",
    "    # Once we calculate the weight and bias gradients, how can we check that they are correct?\n",
    "    # Thankfully there is a way to be sure if our code is correct. We simply use the formal definition of a\n",
    "    # partial derivative -- it is simply a rate of change over an extremely small interval.\n",
    "    # Thus, we can come up with an estimate of our partial. Our calculated partial should match.\n",
    "    \n",
    "    # Each of these methods will select a random partial from its respective gradient array and compare it\n",
    "    # with the estimate. It will print both the calculated value and the estimated value.\n",
    "    # If the two numbers printed are close to the same, then your gradient calculations are correct.\n",
    "    # If they are very different, your calculations are wrong.\n",
    "    # Feel free to use these to test your implementations.\n",
    "    \n",
    "    \n",
    "    # Parameters:\n",
    "    # x: sample image\n",
    "    # y: sample label\n",
    "\n",
    "    def check_V_grad(self, x, y, perturb=0.00001):\n",
    "        randi = (random.randrange(12), random.randrange(10))\n",
    "        self.forward(x)\n",
    "        self.calculate_dL_dVij(randi[0], randi[1], y)\n",
    "        test1 = self.V_grad[randi[0], randi[1]]\n",
    "        self.clear()\n",
    "        loss1 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.V[randi[0], randi[1]] += perturb\n",
    "        loss2 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.V[randi[0], randi[1]] -= perturb\n",
    "        \n",
    "        test2 = (loss2 - loss1) / perturb\n",
    "        \n",
    "        print(test1)\n",
    "        print(test2)\n",
    "        \n",
    "    def check_W_grad(self, x, y, perturb=0.00001):\n",
    "        randi = (random.randrange(784), random.randrange(12))\n",
    "        self.forward(x)\n",
    "        self.calculate_dL_dWij(randi[0], randi[1], y)\n",
    "        test1 = self.W_grad[randi[0], randi[1]]\n",
    "        self.clear()\n",
    "        loss1 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.W[randi[0], randi[1]] += perturb\n",
    "        loss2 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.W[randi[0], randi[1]] -= perturb\n",
    "        \n",
    "        test2 = (loss2 - loss1) / perturb\n",
    "        \n",
    "        print(test1)\n",
    "        print(test2)\n",
    "        \n",
    "    \n",
    "    def check_C_grad(self, x, y, perturb=0.00001):\n",
    "        randi = random.randrange(12)\n",
    "        self.forward(x)\n",
    "        self.calculate_dL_dCj(randi, y)\n",
    "        test1 = self.C_grad[randi]\n",
    "        self.clear()\n",
    "        loss1 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.C[randi] += perturb\n",
    "        loss2 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.C[randi] -= perturb\n",
    "        \n",
    "        test2 = (loss2 - loss1) / perturb\n",
    "        \n",
    "        print(test1)\n",
    "        print(test2)\n",
    "        \n",
    "    def check_B_grad(self, x, y, perturb=0.00001):\n",
    "        randi = random.randrange(10)\n",
    "        self.forward(x)\n",
    "        self.calculate_dL_dBj(randi, y)\n",
    "        test1 = self.B_grad[randi]\n",
    "        self.clear()\n",
    "        loss1 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.B[randi] += perturb\n",
    "        loss2 = self.calculate_loss(x, y)\n",
    "        self.clear()\n",
    "        self.B[randi] -= perturb\n",
    "        \n",
    "        test2 = (loss2 - loss1) / perturb\n",
    "        \n",
    "        print(test1)\n",
    "        print(test2)\n",
    "        \n",
    "        \n",
    "    def clear(self):\n",
    "        # Arrays to hold node values\n",
    "        self.N = np.zeros((784, ))\n",
    "        self.H = np.zeros((12, ))\n",
    "        self.Z = np.zeros((10, ))\n",
    "        \n",
    "        self.W_grad = np.zeros((784, 12))\n",
    "        self.V_grad = np.zeros((12, 10))\n",
    "        self.B_grad = np.zeros((12, ))\n",
    "        self.C_grad = np.zeros((10, ))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
